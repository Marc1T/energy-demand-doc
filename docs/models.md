# Mod√®les de pr√©vision

Dans ce projet, nous avons test√© plusieurs mod√®les de machine learning et de deep learning pour pr√©dire la demande √©lectrique en Espagne √† diff√©rentes √©chelles temporelles (horaire et journali√®re). Chaque mod√®le a √©t√© √©valu√© sur ses performances de pr√©diction, sa robustesse et sa capacit√© √† g√©n√©raliser selon les zones g√©ographiques.

---

## 1. Mod√®les classiques de Machine Learning

### üéØ R√©gression Ridge

- **Principe** : Variante de la r√©gression lin√©aire avec r√©gularisation L2 pour √©viter le surapprentissage.
- **Pourquoi l'utiliser ?** : Simple, rapide et efficace avec des s√©ries temporelles bien pr√©trait√©es.
- **Utilisation dans le projet** :
  - Test√© comme baseline.
  - Bonnes performances sur certaines zones, notamment "Baleares" et "Peninsule_Iberique".
- **Avantages** :
  - Interpr√©table.
  - Faible co√ªt computationnel.
- **Inconv√©nients** :
  - Faible capacit√© √† mod√©liser la non-lin√©arit√©.

üì∑ *[Inclure ici un screenshot de la pr√©diction Ridge vs r√©alit√©]*

---

### üéØ ElasticNet

- **Principe** : Combine r√©gularisation L1 (Lasso) et L2 (Ridge).
- **Pourquoi l'utiliser ?** : Utile quand plusieurs features sont corr√©l√©es.
- **Utilisation dans le projet** : Meilleur mod√®le sur certaines zones comme "Melilla".
- **Avantages** :
  - S√©lectionne automatiquement les variables pertinentes.
- **Limites** :
  - Moins stable sur les petites s√©ries.

---

### üå≥ Random Forest

- **Principe** : Ensemble de plusieurs arbres de d√©cision entra√Æn√©s sur des sous-√©chantillons.
- **Pourquoi l'utiliser ?** : Tr√®s robuste, performant sur des donn√©es non lin√©aires.
- **Utilisation dans le projet** :
  - Meilleur mod√®le horaire dans la majorit√© des zones.
  - √âgalement utilis√© en journalier.
- **Avantages** :
  - Pr√©dictions stables.
  - G√®re les interactions implicites entre variables.
- **Inconv√©nients** :
  - Moins interpr√©table.
  - Plus lent √† entra√Æner.

üì∑ *[Inclure un graphe RF : importance des variables]*

---

### ‚ö° LightGBM

- **Principe** : Boosting d'arbres (Gradient Boosting) tr√®s rapide.
- **Pourquoi l'utiliser ?** : Excellente performance pour les probl√®mes structur√©s.
- **Utilisation dans le projet** : Test√© mais parfois battu par Random Forest.
- **Avantages** :
  - Efficace en entra√Ænement.
  - G√®re bien les s√©ries avec bruit.
- **Limites** :
  - Sensible √† l'overfitting sans r√©glage.

---

### ‚è≥ ARIMA (AutoRegressive Integrated Moving Average)

- **Principe** : Mod√©lisation bas√©e uniquement sur la s√©rie pass√©e.
- **Pourquoi l'utiliser ?** : Baseline statistique fiable.
- **Utilisation dans le projet** :
  - Mod√®le de r√©f√©rence sur s√©ries stationnaires.
  - Moins comp√©titif que les mod√®les ML en cas de forte variabilit√©.
- **Avantages** :
  - Interpr√©table et th√©oriquement solide.
- **Limites** :
  - Ne prend pas en compte les variables exog√®nes.
  - Moins performant sur des s√©ries complexes.

---

## 2. Mod√®les Deep Learning

### üß† LSTM (Long Short-Term Memory)

- **Principe** : R√©seau de neurones r√©currents adapt√© √† la m√©moire √† long terme.
- **Pourquoi l'utiliser ?** : Excellent pour capturer des d√©pendances temporelles lointaines.
- **Utilisation dans le projet** :
  - Pr√©vision de la demande horaire et journali√®re.
  - Exige un preprocessing soign√©.
- **Avantages** :
  - Prend en compte le contexte historique.
- **Inconv√©nients** :
  - Long √† entra√Æner.
  - Sensible aux hyperparam√®tres.

üì∑ *[Inclure courbe d'apprentissage + pr√©diction LSTM vs r√©elle]*

---

### üîÅ GRU (Gated Recurrent Unit)

- **Principe** : Variante simplifi√©e de LSTM.
- **Pourquoi l'utiliser ?** : Moins co√ªteux que LSTM, performances souvent comparables.
- **Utilisation dans le projet** :
  - Utilis√© comme alternative √† LSTM.
- **Avantages** :
  - Moins de param√®tres.
- **Limites** :
  - L√©g√®rement moins performant pour de longues d√©pendances.

---

### üß© CNN-1D

- **Principe** : Applique des filtres convolutifs sur les s√©ries pour d√©tecter des patterns locaux.
- **Pourquoi l'utiliser ?** : Tr√®s bon pour d√©tecter des motifs cycliques.
- **Utilisation dans le projet** :
  - Pr√©vision sur des s√©quences fixes (input sliding window).
- **Avantages** :
  - Rapide et efficace.
- **Limites** :
  - Contexte temporel plus local que LSTM/GRU.

---

### üîÄ CNN-LSTM

- **Principe** : Combine un bloc CNN (extraction de motifs) + LSTM (s√©quence).
- **Pourquoi l'utiliser ?** : Combine les points forts des deux approches.
- **Utilisation dans le projet** :
  - Pr√©vision avanc√©e avec s√©quence d‚Äôentr√©e multi-feature.
- **Avantages** :
  - Tr√®s performant si bien calibr√©.
- **Inconv√©nients** :
  - Co√ªt d'entra√Ænement √©lev√©.

---

## üß™ S√©lection des meilleurs mod√®les

Apr√®s entra√Ænement, chaque mod√®le a √©t√© √©valu√© √† l‚Äôaide de :
- RMSE (Root Mean Squared Error)
- MAE (Mean Absolute Error)
- MAPE (Mean Absolute Percentage Error)

Les meilleurs mod√®les par zone ont √©t√© automatiquement s√©lectionn√©s puis sauvegard√©s :

```text
üìÅ submission/
‚îú‚îÄ‚îÄ Peninsule_Iberique_processed_hourly.parquet
‚îú‚îÄ‚îÄ Melilla_processed_daily.parquet
‚îú‚îÄ‚îÄ features_selected_hourly.csv
‚îú‚îÄ‚îÄ best_models.csv
```

üì∑ *\[Inclure un tableau r√©sum√© des performances par zone]*

---

## ‚úÖ R√©sum√©

Chaque mod√®le a √©t√© test√© de mani√®re rigoureuse, avec :

* Pr√©traitement sp√©cifique par zone.
* S√©lection de features contextuelles.
* Validation crois√©e (CV).
* S√©lection automatique du meilleur algorithme.

‚û°Ô∏è Voir la section [R√©sultats](results.md) pour les scores d√©taill√©s et la robustesse des mod√®les.


